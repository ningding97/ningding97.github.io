<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Ning Ding | publications</title>
<meta name="description" content="">

<!-- Open Graph -->

<meta property="og:site_name" content="" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="/publications/" />
<meta property="og:description" content="publications" />
<meta property="og:image" content="true" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üìü</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>





<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=93OoYW0z85N"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '93OoYW0z85N' });
</script>

    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       Ning Ding
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/awards/">
                awards
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/service/">
                service
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">\* means equal contribution</p>
  </header>

  <article>
    <div class="publications">



  <h2 class="year">Preprint</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="ding2021prompt" class="col-sm-8">
    
      <div class="title">Prompt-Learning for Fine-Grained Entity Typing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chen, Yulin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Han, Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu, Guangwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Pengjun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zhiyuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Juanzi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Hong-Gee, Kim
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv</em>
      
      
        Preprint
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/pdf/2108.10604v1.pdf" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://arxiv.org/pdf/2108.10604v1.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>As an effective approach to tune pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using <i>cloze</i>-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on three fine-grained entity typing benchmarks (with up to 86 classes) under fully supervised, few-shot and zero-shot settings show that prompt-learning methods significantly outperform fine-tuning baselines, especially when the training data is insufficient. </p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="hu2021knowledgeable" class="col-sm-8">
    
      <div class="title">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Hu, Shengding,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Huadong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zhiyuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Juanzi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sun, Maosong
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv</em>
      
      
        Preprint
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2108.02035" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://arxiv.org/pdf/2108.02035.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompt-tuning (KPT), to improve and stabilize prompt-tuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="han2021ptr" class="col-sm-8">
    
      <div class="title">PTR: Prompt Tuning with Rules for Text Classification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Han, Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhao, Weilin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zhiyuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sun, Maosong
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In arXiv</em>
      
      
        Preprint
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2105.11259" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://arxiv.org/pdf/2105.11259.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Fine-tuned pre-trained language models (PLMs) have achieved awesome performance on almost all NLP tasks. By using additional prompts to fine-tune PLMs, we can further stimulate the rich knowledge distributed in PLMs to better serve downstream task. Prompt tuning has achieved promising results on some few-class classification tasks such as sentiment classification and natural language inference. However, manually designing lots of language prompts is cumbersome and fallible. For those auto-generated prompts, it is also expensive and time-consuming to verify their effectiveness in non-few-shot scenarios. Hence, it is challenging for prompt tuning to address many-class classification tasks. To this end, we propose prompt tuning with rules (PTR) for many-class text classi- fication, and apply logic rules to construct prompts with several sub-prompts. In this way, PTR is able to encode prior knowledge of each class into prompt tuning. We conduct experiments on relation classification, a typical many-class classification task, and the results on benchmarks show that PTR can significantly and consistently outperform existing state-of-the-art baselines. This indicates that PTR is a promising approach to take advantage of PLMs for those complicated classification tasks.</p>
    </div>
    
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="ding2021few" class="col-sm-8">
    
      <div class="title">Few-NERD: A Few-shot Named Entity Recognition Dataset</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu, Guangwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chen, Yulin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Xiaobin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Han, Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Pengjun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liu, Zhiyuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Annual Meeting of the Association for Computational Linguistics, <br /> ACL</em>
      
      
        2021
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ningding97.github.io/fewnerd/" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://arxiv.org/pdf/2105.07464.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/thunlp/Few-NERD" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present <font style="font-variant: small-caps">Few-NERD</font>, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. <font style="font-variant: small-caps">Few-NERD</font> consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type.  To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset.  We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that<font style="font-variant: small-caps">Few-NERD</font> is challenging and the problem requires further research. We make <font style="font-variant: small-caps">Few-NERD</font> public at https://ningding97.github.io/fewnerd/ </p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="wang2021cline" class="col-sm-8">
    
      <div class="title">CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Wang, Dong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Piji,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zheng, Hai-Tao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Annual Meeting of the Association for Computational Linguistics, <br /> ACL</em>
      
      
        2021
      
      </div>
    

    <div class="bib">
      
    
    
    
      <a href="https://arxiv.org/abs/2107.00440" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://arxiv.org/pdf/2107.00440.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/kandorm/CLINE" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AI Open</abbr>
    
  
  </div>

  <div id="han2021pre" class="col-sm-8">
    
      <div class="title">Pre-Trained Models: Past, Present and Future</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Han, Xu*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Zhengyan*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gu, Yuxian*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Xiao*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huo, Yuqi*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Qiu, Jiezhong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Liang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Han, Wentao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huang, Minlie,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jin, Qin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lan, Yanyan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zhiyuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lu, Zhiwu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Qiu, Xipeng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Song, Ruihua,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Tang, Jie,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wen, Ji-Rong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuan, Jinhui,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhao, Wayne Xin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Jun, Zhu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In AI Open</em>
      
      
        2021
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://arxiv.org/abs/2106.07139" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://arxiv.org/pdf/2106.07139.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can ef- fectively capture knowledge from massive la- beled and unlabeled data. By storing knowl- edge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a vari- ety of downstream tasks, which has been exten- sively demonstrated via experimental verifica- tion and empirical analysis. It is now the con- sensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre- training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we compre- hensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increas- ing availability of data, towards four impor- tant directions: designing effective architec- tures, utilizing rich contexts, improving com- putational efficiency, and conducting interpre- tation and theoretical analysis. Finally, we dis- cuss a series of open problems and research directions of PTMs, and hope our view can in- spire and advance the future study of PTMs.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="ding2021prototypical" class="col-sm-8">
    
      <div class="title">Prototypical Representation Learning for Relation Extraction</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Xiaobin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fu, Yao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu, Guangwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Rui,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Pengjun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shen, Ying,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Huang, Fei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zhang, Rui
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations, <br />  ICLR</em>
      
      
        2021
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://openreview.net/forum?id=aCgLmfhIy_f" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://openreview.net/forum?id=aCgLmfhIy_f" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Alibaba-NLP/ProtoRE" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recognizing relations between entities is a pivotal task of relational learning. Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise and complicated expressions in human language. This paper aims to learn predictive, interpretable, and robust relation representations from distantly-labeled data that are effective in different settings, including supervised, distantly supervised, and few-shot learning. Instead of solely relying on the supervision from noisy labels, we propose to learn prototypes for each relation from contextual information to best explore the intrinsic semantics of relations. Prototypes are representations in the feature space abstracting the essential semantics of relations between entities in sentences. We learn prototypes based on objectives with clear geometric interpretation, where the prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered at the end of their corresponding prototype vectors on the surface of the ball. This approach allows us to learn meaningful, interpretable prototypes for the final classification. Results on several relation learning tasks show that our model significantly outperforms the previous state-of-the-art models. We further demonstrate the robustness of the encoder and the interpretability of prototypes with extensive experiments.</p>
    </div>
    
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="ding2020coupling" class="col-sm-8">
    
      <div class="title">Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Long, Dingkun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu, Guangwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhu, Muhua,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Pengjun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wang, Xiaobin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zheng, Hai-Tao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Annual Meeting of the Association for Computational Linguistics, <br /> ACL</em>
      
      
        2020
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.aclweb.org/anthology/2020.acl-main.595/" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.acl-main.595/" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Alibaba-NLP/DAAT-CWS" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of ‚ÄúChinese words‚Äù and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain. The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain. 2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information. Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="zhou-etal-2020-hierarchy" class="col-sm-8">
    
      <div class="title">Hierarchy-Aware Global Model for Hierarchical Text Classification</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Zhou, Jie,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ma, Chunping,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Long, Dingkun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu, Guangwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhang, Haoyu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xie, Pengjun,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Liu, Gongshen
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Annual Meeting of the Association for Computational Linguistics, <br /> ACL</em>
      
      
        2020
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.aclweb.org/anthology/2020.acl-main.104" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/2020.acl-main.104" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/Alibaba-NLP/HiAGM" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants. A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features. A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">TKDE</abbr>
    
  
  </div>

  <div id="shen2020modeling" class="col-sm-8">
    
      <div class="title">Modeling Relation Paths for Knowledge Graph Completion</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Shen, Ying,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Yaliang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Yang, Min
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>IEEE Transactions on Knowledge and Data Engineering (TKDE),</em>
      
      
        2020
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ieeexplore.ieee.org/abstract/document/8974254" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/8974254" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Knowledge graphs (KG) often encounter knowledge incompleteness. The path reasoning that predicts the unknown path relation between pairwise entities based on existing facts is one of the most promising approaches to the knowledge graph completion. However, most conventional path reasoning methods exclusively consider the entity description included in fact triples, ignoring both the type information of entities and the interaction between different semantic representations. In this study, we propose a novel method, Type-aware Attentive Path Reasoning (TAPR), to complete the knowledge graph by simultaneously considering KG structural information, textual information, and type information. More specifically, we first leverage types to enrich the representational learning of entities and relationships. Next, we describe a type-level attention to select the most relevant type of given entity in a specific triple without any predefined rules or patterns to reduce the impact of noisy types. After learning the distributed representation of all paths, path-level attention assigns different weights to paths, from which relations among entity pairs are calculated. We conduct a series of experiments on a real-world dataset to demonstrate the effectiveness of TAPR. Experimental results show that our method significantly outperforms all baselines on link prediction and entity prediction tasks.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="lin2020integrating" class="col-sm-8">
    
      <div class="title">Integrating Linguistic Knowledge to Sentence Paraphrase Generation</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Lin, Zibo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Ziran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shen, Ying,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhao, Xiaobin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zheng, Cong-Zhi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In AAAI Conference on Artificial Intelligence, <br /> AAAI</em>
      
      
        2020
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6354" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="/assets/pdf/AAAI2020-para.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/LINMouMouZiBo/KEPN" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Paraphrase generation aims to rewrite a text with different words while keeping the same meaning. Previous work performs the task based solely on the given dataset while ignoring the availability of external linguistic knowledge. However, it is intuitive that a model can generate more expressive and diverse paraphrase with the help of such knowledge. To fill this gap, we propose Knowledge-Enhanced Paraphrase Network (KEPN), a transformer-based framework that can leverage external linguistic knowledge to facilitate paraphrase generation. (1) The model integrates synonym information from the external linguistic knowledge into the paraphrase generator, which is used to guide the decision on whether to generate a new word or replace it with a synonym. (2) To locate the synonym pairs more accurately, we adopt an incremental encoding scheme to incorporate position information of each synonym. Besides, a multi-task architecture is designed to help the framework jointly learn the selection of synonym pairs and the generation of expressive paraphrase. Experimental results on both English and Chinese datasets show that our method significantly outperforms the state-of-the-art approaches in terms of both automatic and human evaluation.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI</abbr>
    
  
  </div>

  <div id="ijcai2020-522" class="col-sm-8">
    
      <div class="title">Infobox-to-text Generation with Tree-like Planning based Attention Network</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Bai, Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Ziran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Shen, Ying,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zheng, Hai-Tao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Joint Conference on
               Artificial Intelligence, <br /> IJCAI</em>
      
      
        2020
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.24963/ijcai.2020/522" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="/assets/pdf/IJCAI2020-infobox.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the problem of infobox-to-text generation that aims to generate a textual description from a key-value table. Representing the input infobox as a sequence, previous neural methods using end-to-end models without order-planning suffer from the problems of incoherence and inadaptability to disordered input. Recent planning-based models only implement static order-planning to guide the generation, which may cause error propagation between planning and generation. To address these issues, we propose a Tree-like PLanning based Attention Network (Tree-PLAN) which leverages both static order-planning and dynamic tuning to guide the generation. A novel tree-like tuning encoder is designed to dynamically tune the static order-plan for better planning by merging the most relevant attributes together layer by layer. Experiments conducted on two datasets show that our model outperforms previous methods on both automatic and human evaluation, and demonstrate that our model has better adaptability to disordered input.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IJCAI</abbr>
    
  
  </div>

  <div id="ijcai2020-523" class="col-sm-8">
    
      <div class="title">Triple-to-Text Generation with an Anchor-to-Prototype Framework</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Li, Ziran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lin, Zibo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shen, Ying
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Joint Conference on
               Artificial Intelligence, <br /> IJCAI</em>
      
      
        2020
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://doi.org/10.24963/ijcai.2020/523" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="/assets/pdf/IJCAI2020-triple.pdf" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Generating a textual description from a set of RDF triplets is a challenging task in natural language generation. Recent neural methods have become the mainstream for this task, which often generate sentences from scratch. However, due to the huge gap between the structured input and the unstructured output, the input triples alone are insufficient to decide an expressive and specific description. In this paper, we propose a novel anchor-to-prototype framework to bridge the gap between structured RDF triples and natural text. The model retrieves a set of prototype descriptions from the training data and extracts writing patterns from them to guide the generation process. Furthermore, to make a more precise use of the retrieved prototypes, we employ a triple anchor that aligns the input triples into groups so as to better match the prototypes. Experimental results on both English and Chinese datasets show that our method significantly outperforms the state-of-the-art baselines in terms of both automatic and manual evaluation, demonstrating the benefit of learning guidance from retrieved prototypes to facilitate triple-to-text generation.</p>
    </div>
    
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="ding-2019-event" class="col-sm-8">
    
      <div class="title">Event Detection with Trigger-Aware Lattice Neural Network</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Ziran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zhiyuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Zheng, Hai-Tao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In The Conference on Empirical Methods in Natural Language Processing, <br /> EMNLP</em>
      
      
        2019
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.aclweb.org/anthology/D19-1033/" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/D19-1033/" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/thunlp/TLNN" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Event detection (ED) aims to locate trigger words in raw text and then classify them into correct event types. In this task, neural net- work based models became mainstream in re- cent years. However, two problems arise when it comes to languages without natural delim- iters, such as Chinese. First, word-based mod- els severely suffer from the problem of word- trigger mismatch, limiting the performance of the methods. In addition, even if trigger words could be accurately located, the ambi- guity of polysemy of triggers could still af- fect the trigger classification stage. To ad- dress the two issues simultaneously, we pro- pose the Trigger-aware Lattice Neural Net- work (TLNN). (1) The framework dynami- cally incorporates word and character informa- tion so that the trigger-word mismatch issue can be avoided. (2) Moreover, for polysemous characters and words, we model all senses of them with the help of an external linguistic knowledge base, so as to alleviate the prob- lem of ambiguous triggers. Experiments on two benchmark datasets show that our model could effectively tackle the two issues and outperforms previous state-of-the-art methods significantly, giving the best results. The source code of this paper can be obtained from https://github.com/thunlp/TLNN.</p>
    </div>
    
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="li-etal-2019-chinese" class="col-sm-8">
    
      <div class="title">Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Li, Ziran*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Ding, Ning*</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Zhiyuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zheng, Hai-Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shen, Ying
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Annual Meeting of the Association for Computational Linguistics, <br /> ACL</em>
      
      
        2019
      
      </div>
    

    <div class="bib">
      
    
      <a class="abstract btn-abs btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://www.aclweb.org/anthology/P19-1430" class="btn-html btn-sm z-depth-0" role="button" target="_blank">Link</a>
    
    
      
      <a href="https://www.aclweb.org/anthology/P19-1430" class="btn-pdf btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/thunlp/Chinese_NRE" class="btn-code btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    <a href="https://github.com/thunlp/Chinese_NRE/tree/master/data/FinRE" class="btn-data btn-sm z-depth-0" role="button" target="_blank">Data</a>
  
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Chinese relation extraction is conducted using neural networks with either character-based or word-based inputs, and most existing methods typically suffer from segmentation errors and ambiguity of polysemy. To address the issues, we propose a multi-grained lattice framework (MG lattice) for Chinese relation extraction to take advantage of multi-grained language information and external linguistic knowledge. In this framework, (1) we incorporate word-level information into character sequence inputs so that segmentation errors can be avoided. (2) We also model multiple senses of polysemous words with the help of external linguistic knowledge, so as to alleviate polysemy ambiguity. Experiments on three real-world datasets in distinct domains show consistent and significant superiority and robustness of our model, as compared with other baselines. We will release the source code of this paper in the future.</p>
    </div>
    
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Ning  Ding.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>

    
    
    Last updated: October 22, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
